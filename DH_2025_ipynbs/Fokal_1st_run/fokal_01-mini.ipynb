{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7141783-7817-4e7c-ad1e-50fe75cf6dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "from statsmodels.stats.contingency_tables import mcnemar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66fbceb6-a14f-47f3-b08c-86a1595a093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89314e97-8eaf-49cc-9246-c86e4f09d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df995423-2177-4863-91db-5c5b4b74964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a628383-5a13-4bfc-9df4-b067b6ec4dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anno = pd.read_csv('DH_2025_Anno_DEU_2nd_run.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82bccc58-19d7-4456-8178-09093d414624",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anno['Fokalisierung'] = df_anno['Fokalisierung'].replace({\n",
    "    'intern': 'internal',\n",
    "    'extern': 'external',\n",
    "    'null': 'zero',\n",
    "    None: 'zero',\n",
    "    np.nan: 'zero'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a872635-ea48-4780-9256-9e983ea3a99b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Autor</th>\n",
       "      <th>Titel</th>\n",
       "      <th>Absatz</th>\n",
       "      <th>Fokalisierung</th>\n",
       "      <th>Kommentar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Schiller</td>\n",
       "      <td>Der Vebrecher aus verlorener Ehre</td>\n",
       "      <td>In der ganzen Geschichte des Menschen ist kein...</td>\n",
       "      <td>zero</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Schiller</td>\n",
       "      <td>Der Vebrecher aus verlorener Ehre</td>\n",
       "      <td>Es ist etwas so Einförmiges und doch wieder so...</td>\n",
       "      <td>zero</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Schiller</td>\n",
       "      <td>Der Vebrecher aus verlorener Ehre</td>\n",
       "      <td>Ich weiß, daß von den besten Geschichtschreibe...</td>\n",
       "      <td>internal</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Schiller</td>\n",
       "      <td>Der Vebrecher aus verlorener Ehre</td>\n",
       "      <td>Der Held muß kalt werden wie der Leser, oder, ...</td>\n",
       "      <td>zero</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tieck</td>\n",
       "      <td>Die beiden merkwürdigsten Tage aus Siegmunds L...</td>\n",
       "      <td>Es war schon gegen Abend, als ein Wagen vor de...</td>\n",
       "      <td>internal</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Autor                                              Titel  \\\n",
       "0  Schiller                  Der Vebrecher aus verlorener Ehre   \n",
       "1  Schiller                  Der Vebrecher aus verlorener Ehre   \n",
       "2  Schiller                  Der Vebrecher aus verlorener Ehre   \n",
       "3  Schiller                  Der Vebrecher aus verlorener Ehre   \n",
       "4     Tieck  Die beiden merkwürdigsten Tage aus Siegmunds L...   \n",
       "\n",
       "                                              Absatz Fokalisierung  Kommentar  \n",
       "0  In der ganzen Geschichte des Menschen ist kein...          zero        NaN  \n",
       "1  Es ist etwas so Einförmiges und doch wieder so...          zero        NaN  \n",
       "2  Ich weiß, daß von den besten Geschichtschreibe...      internal        NaN  \n",
       "3  Der Held muß kalt werden wie der Leser, oder, ...          zero        NaN  \n",
       "4  Es war schon gegen Abend, als ein Wagen vor de...      internal        NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_anno.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3fbefde-5a2a-4237-a347-9eea73e9befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('MY_OPENAI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c641fbf-c363-4333-b161-c9ef54003893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"o1-mini\"):  \n",
    "    client = OpenAI(api_key=api_key)\n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt,\n",
    "        }],\n",
    "        model=model,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4801e14-5434-4239-b884-5392bd6bf1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_basic = \"\"\"\n",
    "### Instruction\n",
    "Your task is to classify the focalization of the following sentence\n",
    "\n",
    "###\n",
    "Only respond with one word representing the mode of focalization, do NOT give explenations or generate more text \n",
    "Sentence: '''{text}''''\n",
    "Label:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f88bc732-1ab7-4c8e-9ca2-d6659a271388",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_labels = \"\"\"\n",
    "### Instruction\n",
    "Your task is to classify the focalization of the following sentence\n",
    "\n",
    "### Labels\n",
    "There are three modes of focalization:\n",
    "- internal\n",
    "- external\n",
    "- zero\n",
    "\n",
    "####\n",
    "Only respond with one word representing the mode of focalization, do NOT give explenations or generate more text \n",
    "Sentence: '''{text}''' \n",
    "Label: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a28efea-cd61-4b50-8e9b-8cf27d39daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_redefin = \"\"\"\n",
    "### Instruction\n",
    "Your task is to classify the focalization of the following sentence\n",
    "\n",
    "### Labels\n",
    "There are three modes of focalization:\n",
    "- internal: A text passage is internally focalized precisely when a perceptual process is part of the depicted event and is presented from the perspective of a character.\n",
    "- external: A text passage is externally focalized precisely when a perceptual process is part of the depicted event and could be presented from the perspective of a character.\n",
    "- zero: A text passage is zero focalized precisely when circumstances of the narrated world are described as if they were independent of a particular perceptual process of a person or are not possible for a person to perceive synchronously.\n",
    "\n",
    "####\n",
    "Only respond with one word representing the mode of focalization, do NOT give explenations or generate more text  \n",
    "Sentence: '''{text}''' \n",
    "Label: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73534894-2a90-40b2-acc4-d81a9abab0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_meta = \"\"\"\n",
    "### Instruction\n",
    "Your task is to classify the focalization of the following sentence\n",
    "\n",
    "### Labels\n",
    "There are three modes of focalization:\n",
    "- internal: A text passage is internally focalized precisely when a perceptual process is part of the depicted event and is presented from the perspective of a character.\n",
    "- external: A text passage is externally focalized precisely when a perceptual process is part of the depicted event and could be presented from the perspective of a character.\n",
    "- zero: A text passage is zero focalized precisely when circumstances of the narrated world are described as if they were independent of a particular perceptual process of a person or are not possible for a person to perceive synchronously. \n",
    "These definitions are redefinitions of the standard understanding of focalization.\n",
    "\n",
    "####\n",
    "Only respond with one word representing the mode of focalization, do NOT give explenations or generate more text \n",
    "Sentence: '''{text}''' \n",
    "Label: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7aedc23c-95fb-41e6-8f42-4bc779f5c3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_templates = [prompt_labels, prompt_redefin, prompt_meta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbc03d8f-dc77-4569-bb48-b131a98d65d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompts_and_predictions(df, prompt_templates):\n",
    "    \"\"\"\n",
    "    Evaluiert verschiedene Prompt-Templates und berechnet Metriken für die Vorhersagen.\n",
    "    \n",
    "    Args:\n",
    "        df: Pandas DataFrame mit den Spalten 'Absatz' und 'Fokalisierung'\n",
    "        prompt_templates: Liste der Prompt-Templates\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame mit den Evaluierungsmetriken für jeden Prompt\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Iteration über die Prompt-Templates\n",
    "    for prompt_idx, template in enumerate(prompt_templates):\n",
    "        print(f\"Verarbeite Prompt-Template {prompt_idx + 1}/{len(prompt_templates)}\")\n",
    "        \n",
    "        # Neue Spalte für Vorhersagen erstellen\n",
    "        df[f'Prediction_{prompt_idx}'] = None\n",
    "        \n",
    "        # Iteration über die Zeilen des DataFrames\n",
    "        for idx, row in df.iterrows():\n",
    "            # Prompt erstellen durch Einsetzen des Absatzes in das Template\n",
    "            prompt = template.format(text=row['Absatz'])\n",
    "            \n",
    "            # Vorhersage mit Mixtral-Modell\n",
    "            prediction = get_completion(prompt)\n",
    "            first_word = prediction.split()[0]\n",
    "            first_word = first_word.lower()\n",
    "            print(first_word)\n",
    "            \n",
    "            # Vorhersage speichern\n",
    "            df.at[idx, f'Prediction_{prompt_idx}'] = first_word\n",
    "        \n",
    "        #print(df)\n",
    "        \n",
    "        # Metriken berechnen\n",
    "        metrics = {\n",
    "            'Prompt': f'Template_{prompt_idx}',\n",
    "            'F1-Score': f1_score(df['Fokalisierung'], df[f'Prediction_{prompt_idx}'], average='weighted'),\n",
    "            'Recall': recall_score(df['Fokalisierung'], df[f'Prediction_{prompt_idx}'], average='weighted'),\n",
    "            'Precision': precision_score(df['Fokalisierung'], df[f'Prediction_{prompt_idx}'], average='weighted'),\n",
    "            'Accuracy': accuracy_score(df['Fokalisierung'], df[f'Prediction_{prompt_idx}'])\n",
    "        }\n",
    "        \n",
    "        results.append(metrics)\n",
    "    \n",
    "    # Ergebnisse in DataFrame umwandeln\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return df, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35569bc4-db87-4563-99b3-3c0b21a5cdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verarbeite Prompt-Template 1/3\n",
      "zero\n",
      "zero\n",
      "internal\n",
      "zero\n",
      "external\n",
      "external\n",
      "external\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "zero\n",
      "external\n",
      "zero\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "external\n",
      "internal\n",
      "external\n",
      "external\n",
      "zero\n",
      "internal\n",
      "external\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "zero\n",
      "external\n",
      "external\n",
      "zero\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "external\n",
      "zero\n",
      "internal\n",
      "external\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "external\n",
      "external\n",
      "external\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "external\n",
      "external\n",
      "external\n",
      "internal\n",
      "external\n",
      "external\n",
      "zero\n",
      "zero\n",
      "external\n",
      "zero\n",
      "external\n",
      "external\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "external\n",
      "zero\n",
      "zero\n",
      "external\n",
      "zero\n",
      "external\n",
      "external\n",
      "external\n",
      "external\n",
      "external\n",
      "external\n",
      "external\n",
      "external\n",
      "external\n",
      "external\n",
      "internal\n",
      "external\n",
      "internal\n",
      "external\n",
      "zero\n",
      "external\n",
      "external\n",
      "external\n",
      "internal\n",
      "external\n",
      "zero\n",
      "Verarbeite Prompt-Template 2/3\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "internal\n",
      "external\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "zero\n",
      "zero\n",
      "external\n",
      "zero\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "external\n",
      "internal\n",
      "zero\n",
      "internal\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "external\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "zero\n",
      "zero\n",
      "external\n",
      "external\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "external\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "external\n",
      "external\n",
      "external\n",
      "external\n",
      "external\n",
      "external\n",
      "zero\n",
      "zero\n",
      "external\n",
      "zero\n",
      "internal\n",
      "internal\n",
      "external\n",
      "internal\n",
      "internal\n",
      "zero\n",
      "external\n",
      "external\n",
      "external\n",
      "external\n",
      "internal\n",
      "external\n",
      "external\n",
      "zero\n",
      "zero\n",
      "external\n",
      "external\n",
      "external\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "zero\n",
      "zero\n",
      "internal\n",
      "internal\n",
      "external\n",
      "internal\n",
      "zero\n",
      "zero\n",
      "Verarbeite Prompt-Template 3/3\n",
      "zero\n",
      "zero\n",
      "internal\n",
      "zero\n",
      "external\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "zero\n",
      "zero\n",
      "external\n",
      "zero\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "external\n",
      "external\n",
      "zero\n",
      "internal\n",
      "internal\n",
      "zero\n",
      "internal\n",
      "zero\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "external\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "zero\n",
      "zero\n",
      "external\n",
      "external\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "external\n",
      "zero\n",
      "internal\n",
      "internal\n",
      "external\n",
      "external\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "zero\n",
      "external\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "external\n",
      "internal\n",
      "external\n",
      "external\n",
      "external\n",
      "zero\n",
      "external\n",
      "internal\n",
      "zero\n",
      "internal\n",
      "internal\n",
      "internal\n",
      "zero\n",
      "external\n",
      "external\n",
      "zero\n",
      "external\n",
      "internal\n",
      "external\n",
      "zero\n"
     ]
    }
   ],
   "source": [
    "results_o1mini, test = evaluate_prompts_and_predictions(df_anno, prompt_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5899b9f8-6523-48a7-aeb6-5ab85e677609",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_o1mini.to_json(\"DH_o1mini_results\", orient=\"records\", indent=4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfce498a-b205-4602-af89-f8debe982b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Template_0</td>\n",
       "      <td>0.586123</td>\n",
       "      <td>0.566038</td>\n",
       "      <td>0.642890</td>\n",
       "      <td>0.566038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Template_1</td>\n",
       "      <td>0.635171</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.657107</td>\n",
       "      <td>0.622642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Template_2</td>\n",
       "      <td>0.621288</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.629142</td>\n",
       "      <td>0.622642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Prompt  F1-Score    Recall  Precision  Accuracy\n",
       "0  Template_0  0.586123  0.566038   0.642890  0.566038\n",
       "1  Template_1  0.635171  0.622642   0.657107  0.622642\n",
       "2  Template_2  0.621288  0.622642   0.629142  0.622642"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a1da50-1be9-41b1-9107-9990732e3ab9",
   "metadata": {},
   "source": [
    "## Statistischer Test 1: McNemar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e5666ca-d43b-45ae-b267-7235e997e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zum Vergleichen der Vorhersagen mit dem Goldstandard\n",
    "def compare_with_gold(predictions, gold_standard):\n",
    "    return [pred == gold for pred, gold in zip(predictions, gold_standard)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7987c436-2f71-43c7-9fbb-e0d696fa032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1 = compare_with_gold(results_o1mini[\"Prediction_0\"], df_anno[\"Fokalisierung\"])\n",
    "results_2 = compare_with_gold(results_o1mini[\"Prediction_1\"], df_anno[\"Fokalisierung\"])\n",
    "results_3 = compare_with_gold(results_o1mini[\"Prediction_2\"], df_anno[\"Fokalisierung\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76f23597-12d4-42fe-8621-df290d761138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contingency_table(results_a, results_b):\n",
    "    both_correct = sum(a and b for a, b in zip(results_a, results_b))\n",
    "    only_a_correct = sum(a and not b for a, b in zip(results_a, results_b))\n",
    "    only_b_correct = sum(b and not a for a, b in zip(results_a, results_b))\n",
    "    both_incorrect = sum(not a and not b for a, b in zip(results_a, results_b))\n",
    "    return np.array([[both_correct, only_a_correct],\n",
    "                     [only_b_correct, both_incorrect]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b75d38b0-fe8d-47fc-9a1c-60ffce713194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mcnemar_test(results_a, results_b):\n",
    "    table = create_contingency_table(results_a, results_b)\n",
    "    return mcnemar(table, exact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6c323ef-36de-4879-af6b-ab8bea7a6556",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1_2 = run_mcnemar_test(results_1, results_2)\n",
    "test_1_3 = run_mcnemar_test(results_1, results_3)\n",
    "test_2_3 = run_mcnemar_test(results_2, results_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d4cd6f2-9ecc-491c-9ab9-056dac8fc5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "McNemar-Test Ergebnisse:\n",
      "Template 1 vs Template 2: p-Wert = 0.4050322461407632\n",
      "Template 1 vs Template 3: p-Wert = 0.37708558747544885\n",
      "Template 2 vs Template 3: p-Wert = 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"McNemar-Test Ergebnisse:\")\n",
    "print(f\"Template 1 vs Template 2: p-Wert = {test_1_2.pvalue}\")\n",
    "print(f\"Template 1 vs Template 3: p-Wert = {test_1_3.pvalue}\")\n",
    "print(f\"Template 2 vs Template 3: p-Wert = {test_2_3.pvalue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611ff71d-3a4c-4be9-8b1a-f63471269614",
   "metadata": {},
   "source": [
    "## Statistischer Test 2: Paired-ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1683a4e0-7e8f-4344-9b4c-ea863a85bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_statistical_significance(df, results_df):\n",
    "    \"\"\"\n",
    "    Analysiert die statistische Signifikanz der Unterschiede zwischen F1-Scores verschiedener Prompts\n",
    "    \n",
    "    Args:\n",
    "        df: Original DataFrame mit den Vorhersagen\n",
    "        results_df: DataFrame mit den Evaluierungsmetriken aus evaluate_prompts_and_predictions\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame mit den Ergebnissen der statistischen Tests\n",
    "    \"\"\"\n",
    "    \n",
    "    # Liste aller Prompt-Kombinationen erstellen\n",
    "    prompt_combinations = list(combinations(results_df['Prompt'], 2))\n",
    "    \n",
    "    # Ergebnisse speichern\n",
    "    test_results = []\n",
    "    \n",
    "    for prompt1, prompt2 in prompt_combinations:\n",
    "        # Indizes extrahieren\n",
    "        idx1 = int(prompt1.split('_')[1])\n",
    "        idx2 = int(prompt2.split('_')[1])\n",
    "        \n",
    "        # F1-Scores für jeden einzelnen Fall berechnen\n",
    "        f1_scores_1 = []\n",
    "        f1_scores_2 = []\n",
    "        \n",
    "        # Für jede Instanz einzeln F1-Score berechnen\n",
    "        for idx in df.index:\n",
    "            true = df.loc[idx, 'Fokalisierung']\n",
    "            pred1 = df.loc[idx, f'Prediction_{idx1}']\n",
    "            pred2 = df.loc[idx, f'Prediction_{idx2}']\n",
    "            \n",
    "            # Einzelne F1-Scores berechnen (1 für korrekt, 0 für falsch)\n",
    "            f1_scores_1.append(1 if true == pred1 else 0)\n",
    "            f1_scores_2.append(1 if true == pred2 else 0)\n",
    "        \n",
    "        # Verschiedene statistische Tests durchführen\n",
    "        \n",
    "        # 1. Paired t-test\n",
    "        t_stat, t_pvalue = stats.ttest_rel(f1_scores_1, f1_scores_2)\n",
    "        \n",
    "        # 2. Wilcoxon signed-rank test\n",
    "        w_stat, w_pvalue = stats.wilcoxon(f1_scores_1, f1_scores_2)\n",
    "        \n",
    "        # Mittlere Differenz berechnen\n",
    "        mean_diff = np.mean(f1_scores_1) - np.mean(f1_scores_2)\n",
    "        \n",
    "        # Ergebnisse speichern\n",
    "        test_results.append({\n",
    "            'Prompt_Comparison': f'{prompt1} vs {prompt2}',\n",
    "            'Mean_Difference': mean_diff,\n",
    "            'T_Test_p_value': t_pvalue,\n",
    "            'Wilcoxon_p_value': w_pvalue,\n",
    "            'Significant_0.05': any(p < 0.05 for p in [t_pvalue, w_pvalue]),\n",
    "            'Significant_0.01': any(p < 0.01 for p in [t_pvalue, w_pvalue])\n",
    "        })\n",
    "    \n",
    "    # Ergebnisse in DataFrame umwandeln\n",
    "    results_df = pd.DataFrame(test_results)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "658a654d-97a7-4a2a-8e67-0ab931f06b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = analyze_statistical_significance(df_anno, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27b818dd-5db0-4348-a39e-56e3c00b3909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt_Comparison</th>\n",
       "      <th>Mean_Difference</th>\n",
       "      <th>T_Test_p_value</th>\n",
       "      <th>Wilcoxon_p_value</th>\n",
       "      <th>Significant_0.05</th>\n",
       "      <th>Significant_0.01</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Template_0 vs Template_1</td>\n",
       "      <td>-0.056604</td>\n",
       "      <td>0.319609</td>\n",
       "      <td>0.317311</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Template_0 vs Template_2</td>\n",
       "      <td>-0.056604</td>\n",
       "      <td>0.290992</td>\n",
       "      <td>0.288844</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Template_1 vs Template_2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Prompt_Comparison  Mean_Difference  T_Test_p_value  \\\n",
       "0  Template_0 vs Template_1        -0.056604        0.319609   \n",
       "1  Template_0 vs Template_2        -0.056604        0.290992   \n",
       "2  Template_1 vs Template_2         0.000000        1.000000   \n",
       "\n",
       "   Wilcoxon_p_value  Significant_0.05  Significant_0.01  \n",
       "0          0.317311             False             False  \n",
       "1          0.288844             False             False  \n",
       "2          1.000000             False             False  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce41e57a-e04d-494f-bc9e-4684c1b14312",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
